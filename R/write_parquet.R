write_dashboard_parquet = function(metadatadir = c(),
                                   params_output = c(),
                                   verbose = TRUE) {
  # Produce a single consolidated Parquet file from the CSV reports

  # generated by GGIR, ready for consumption by a DuckDB-WASM dashboard.
  #
  # This function is called at the end of GGIR() when

  # params_output$save_dashboard_parquet is TRUE.
  # It reads the already-generated CSVs, joins them, cleans column names,
  # casts types, embeds the variable dictionary as Parquet key-value metadata,
  # and writes results/ggir_results.parquet.

  results_dir = paste0(metadatadir, "/results")
  if (!dir.exists(results_dir)) {
    warning("\nNo results directory found. Skipping Parquet export.", call. = FALSE)
    return(invisible(NULL))
  }

  # ---------------------------------------------------------------
  # Helper: clean column names to be SQL-friendly
  # ---------------------------------------------------------------
  clean_colnames = function(x) {
    x = tolower(x)
    x = gsub("[^a-z0-9_]", "_", x)    # replace special chars with _
    x = gsub("_+", "_", x)             # collapse multiple underscores
    x = gsub("^_|_$", "", x)           # trim leading/trailing _
    x
  }

  # ---------------------------------------------------------------
  # Helper: safely read a CSV, return NULL if not found
  # ---------------------------------------------------------------
  safe_read_csv = function(filepath) {
    if (length(filepath) == 0 || !file.exists(filepath[1])) return(NULL)
    tryCatch(
      data.table::fread(filepath[1], data.table = FALSE),
      error = function(e) {
        if (verbose) warning(paste0("\nCould not read: ", filepath[1], " - ", e$message), call. = FALSE)
        NULL
      }
    )
  }

  # ---------------------------------------------------------------
  # 1. Read Part 5 day summary (the most granular level)
  #    Glob for all threshold combos (e.g. part5_daysummary_full_MM_*.csv)
  # ---------------------------------------------------------------
  p5_files = list.files(paste0(results_dir, "/QC"),
                        pattern = "^part5_daysummary_full_.*\\.csv$",
                        full.names = TRUE)
  if (length(p5_files) == 0) {
    warning("\nNo Part 5 day summary CSVs found. Skipping Parquet export.", call. = FALSE)
    return(invisible(NULL))
  }

  # Read and row-bind all Part 5 day summary files
  p5_list = lapply(p5_files, function(f) {
    df = safe_read_csv(f)
    if (!is.null(df) && nrow(df) > 0) df else NULL
  })
  p5_list = p5_list[!vapply(p5_list, is.null, logical(1))]
  if (length(p5_list) == 0) {
    warning("\nPart 5 day summary CSVs are empty. Skipping Parquet export.", call. = FALSE)
    return(invisible(NULL))
  }
  p5 = do.call(rbind, p5_list)

  # ---------------------------------------------------------------
  # 2. Read Part 4 night summary (per-night sleep data)
  # ---------------------------------------------------------------
  p4_file = paste0(results_dir, "/part4_nightsummary_sleep_cleaned.csv")
  p4 = safe_read_csv(p4_file)

  # ---------------------------------------------------------------
  # 3. Read Part 2 day summary (daily activity L5/M5/MVPA)
  # ---------------------------------------------------------------
  p2day_file = paste0(results_dir, "/part2_daysummary.csv")
  p2day = safe_read_csv(p2day_file)

  # ---------------------------------------------------------------
  # 4. Read Part 2 person summary (recording metadata)
  # ---------------------------------------------------------------
  p2_file = paste0(results_dir, "/part2_summary.csv")
  p2 = safe_read_csv(p2_file)

  # ---------------------------------------------------------------
  # 5. Read data quality report
  # ---------------------------------------------------------------
  qc_file = paste0(results_dir, "/QC/data_quality_report.csv")
  qc = safe_read_csv(qc_file)

  # ---------------------------------------------------------------
  # 6. Read variable dictionary for Parquet metadata
  # ---------------------------------------------------------------
  dict_file = paste0(results_dir, "/variableDictionary/part5_dictionary_daysummary_full.csv")
  dict = safe_read_csv(dict_file)

  # ---------------------------------------------------------------
  # 7. Build the consolidated data frame
  #    Start with Part 5 (most granular), then left-join others
  # ---------------------------------------------------------------
  consolidated = p5

  # Join Part 4 sleep data by ID + calendar_date
  if (!is.null(p4) && nrow(p4) > 0) {
    # Identify Part 4 columns that are NOT already in Part 5
    p4_unique_cols = setdiff(names(p4), names(consolidated))
    join_keys = c("ID", "calendar_date")
    p4_subset = p4[, c(intersect(join_keys, names(p4)), p4_unique_cols), drop = FALSE]
    if (all(join_keys %in% names(p4_subset)) && all(join_keys %in% names(consolidated))) {
      consolidated = merge(consolidated, p4_subset,
                           by = join_keys, all.x = TRUE, suffixes = c("", "_p4"))
    }
  }

  # Join Part 2 day-level data by ID + calendar_date
  if (!is.null(p2day) && nrow(p2day) > 0) {
    p2day_unique_cols = setdiff(names(p2day), names(consolidated))
    join_keys = c("ID", "calendar_date")
    p2day_subset = p2day[, c(intersect(join_keys, names(p2day)), p2day_unique_cols), drop = FALSE]
    if (all(join_keys %in% names(p2day_subset)) && all(join_keys %in% names(consolidated))) {
      consolidated = merge(consolidated, p2day_subset,
                           by = join_keys, all.x = TRUE, suffixes = c("", "_p2day"))
    }
  }

  # Join Part 2 person-level metadata by ID (one row per person, repeated)
  if (!is.null(p2) && nrow(p2) > 0) {
    p2_unique_cols = setdiff(names(p2), names(consolidated))
    join_key = "ID"
    p2_subset = p2[, c(join_key, p2_unique_cols), drop = FALSE]
    if (join_key %in% names(p2_subset) && join_key %in% names(consolidated)) {
      consolidated = merge(consolidated, p2_subset,
                           by = join_key, all.x = TRUE, suffixes = c("", "_p2"))
    }
  }

  # Join QC data by filename
  if (!is.null(qc) && nrow(qc) > 0) {
    qc_unique_cols = setdiff(names(qc), names(consolidated))
    join_key = "filename"
    qc_subset = qc[, c(join_key, qc_unique_cols), drop = FALSE]
    if (join_key %in% names(qc_subset) && join_key %in% names(consolidated)) {
      consolidated = merge(consolidated, qc_subset,
                           by = join_key, all.x = TRUE, suffixes = c("", "_qc"))
    }
  }

  if (nrow(consolidated) == 0) {
    warning("\nConsolidated data is empty. Skipping Parquet export.", call. = FALSE)
    return(invisible(NULL))
  }

  # ---------------------------------------------------------------
  # 8. Clean column names for SQL-friendly access
  # ---------------------------------------------------------------
  names(consolidated) = clean_colnames(names(consolidated))

  # Deduplicate any identical column names that may have arisen from merges
  dupes = duplicated(names(consolidated))
  if (any(dupes)) {
    names(consolidated)[dupes] = paste0(names(consolidated)[dupes], "_dup",
                                        seq_len(sum(dupes)))
  }

  # ---------------------------------------------------------------
  # 9. Cast types for Parquet
  # ---------------------------------------------------------------

  # calendar_date -> Date
  if ("calendar_date" %in% names(consolidated)) {
    consolidated$calendar_date = tryCatch(
      as.Date(consolidated$calendar_date),
      error = function(e) consolidated$calendar_date
    )
  }

  # Boolean columns
  bool_cols = c("daysleeper", "sleeplog_used", "acc_available",
                "file_corrupt", "file_too_short", "use_temperature")
  for (col in bool_cols) {
    if (col %in% names(consolidated)) {
      consolidated[[col]] = as.logical(as.numeric(consolidated[[col]]))
    }
  }

  # Integer columns (counts, codes, nights, bouts)
  int_patterns = c("^n_", "^nbouts_", "^nblocks_", "cleaning_?code",
                    "night_number", "window_number", "measurement_?day",
                    "page", "n_hours_ignored")
  for (pat in int_patterns) {
    matching = grep(pat, names(consolidated), value = TRUE)
    for (col in matching) {
      if (is.character(consolidated[[col]]) || is.numeric(consolidated[[col]])) {
        consolidated[[col]] = tryCatch(
          as.integer(consolidated[[col]]),
          warning = function(w) consolidated[[col]],
          error = function(e) consolidated[[col]]
        )
      }
    }
  }

  # ---------------------------------------------------------------
  # 10. Build Parquet key-value metadata from dictionary
  # ---------------------------------------------------------------
  kv_metadata = list(
    ggir_export = "consolidated_dashboard",
    created_at = as.character(Sys.time())
  )
  if (!is.null(dict) && nrow(dict) > 0 &&
      "Variable" %in% names(dict) && "Definition" %in% names(dict)) {
    for (i in seq_len(nrow(dict))) {
      clean_name = clean_colnames(dict$Variable[i])
      kv_metadata[[clean_name]] = dict$Definition[i]
    }
  }

  # ---------------------------------------------------------------
  # 11. Write Parquet file
  # ---------------------------------------------------------------
  parquet_path = paste0(results_dir, "/ggir_results.parquet")

  # Convert to Arrow table so we can attach key-value metadata
  tbl = arrow::arrow_table(consolidated)
  tbl$metadata = kv_metadata

  arrow::write_parquet(tbl, parquet_path)

  if (verbose) {
    cat(paste0("\n  Parquet file written: ", parquet_path))
    cat(paste0("\n  Rows: ", nrow(consolidated),
               ", Columns: ", ncol(consolidated), "\n"))
  }

  invisible(parquet_path)
}
